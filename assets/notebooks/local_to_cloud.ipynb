{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "The following are the requirements to follow allong with this guide:\n",
    "\n",
    "1. A Google Cloud Platform proyect with billing enabled.\n",
    "2. Conda installed in your system.\n",
    "3. Docker installed in your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set up\n",
    "\n",
    "In this section we will set up the environment for the notebook.\n",
    "1. Create a conda environment with the required packages. Then you will need to activate the environment and use it for the kernel of the notebook.\n",
    "2. Authenticate with the Google Cloud Platform. You will need to do this from the terminal as the notebook does not support the authentication process. \n",
    "3. Obtain local Application Default Credentials for the Google Cloud Platform. This also needs to be done from the terminal.\n",
    "\n",
    "The following cells will guide you through the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile environment.yml\n",
    "name: gcp-prediction-env\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python==3.10\n",
    "  - jupyter\n",
    "  - pandas\n",
    "  - conda-forge::scikit-learn\n",
    "  - numpy\n",
    "  - matplotlib\n",
    "  - seaborn\n",
    "  - scikit-learn\n",
    "  - xgboost>=1.7.0, <2.0.0\n",
    "  - conda-forge::google-cloud-sdk\n",
    "  - pyarrow\n",
    "  - db-dtypes\n",
    "  - pip\n",
    "  - pip:\n",
    "    - google-cloud-aiplatform[prediction]\n",
    "    - fastapi[standard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: The `%%writefile` magic command will write the content of the cell to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if we are in the right conda environment\n",
    "import json\n",
    "conda_info= !conda info --json\n",
    "conda_info = json.loads(''.join(conda_info))\n",
    "conda_env = conda_info['active_prefix_name']\n",
    "if conda_env == 'gcp-prediction-env':\n",
    "    print('Conda environment is set up correctly!')\n",
    "else:\n",
    "    raise Exception('Please use the conda environment gcp-prediction-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if `gcloud auth login` has been run\n",
    "active_string = \"*\" # represents the currently active account\n",
    "gcloud_auth = !gcloud auth list | grep \"$active_string\"\n",
    "gcloud_auth = gcloud_auth[-1]\n",
    "if active_string in gcloud_auth:\n",
    "    account = gcloud_auth.split(' ')[-1]\n",
    "    print(f'Active account is {account}')\n",
    "    print('gcloud has been authenticated up correctly!')\n",
    "else:\n",
    "    raise Exception('Please run `gcloud auth login` from the terminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is an ADC for gcp\n",
    "gcloud_adk = !gcloud auth application-default print-access-token\n",
    "if 'ERROR' not in gcloud_adk[-1]:\n",
    "    print('ADC has been set up correctly!')\n",
    "else:\n",
    "    raise Exception('Please run `gcloud auth application-default login` from the terminal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "This section defines some global variables that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"my-project\" # @param {type:\"string\"}\n",
    "LOCATION = \"us-west1\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"my-project\":\n",
    "    PROJECT_ID = !gcloud config get-value project\n",
    "    PROJECT_ID=PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = \"iris-artifact-repo\"\n",
    "image = \"xgboost-predictor\"\n",
    "image_uri = f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{repository}/{image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project $PROJECT_ID --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42 # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This section imports the required libraries for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Model training\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# HTTP Server and  making requests\n",
    "import subprocess\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# BigQuery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "#aiplatform\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "#logging\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "\n",
    "This sections defines some utility functions that will be used throughout the notebook. Reviewing them is not required but it is recommended to understand what is happening in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_variable_in_file(filepath: str, variable_name: str, variable_value: str) -> None:\n",
    "    \"\"\"\n",
    "    Replaces all occurrences of a variable name with a specified value in a given file.\n",
    "\n",
    "    Parameters:\n",
    "    filepath (str): The path to the file where the replacement should occur.\n",
    "    variable_name (str): The name of the variable to be replaced.\n",
    "    variable_value (str): The value to replace the variable with.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filepath, \"rt\") as file:\n",
    "        s = file.read()\n",
    "\n",
    "    s = re.sub(rf\"{variable_name}\", f'\"{variable_value}\"', s)\n",
    "\n",
    "    with open(filepath, \"wt\") as file:\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bigquery(data_test: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to a BigQuery table.\n",
    "\n",
    "    Parameters:\n",
    "    data_test (pd.DataFrame): The DataFrame containing the data to be uploaded.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    dataset_name = \"iris_predictor\"\n",
    "    table_name = \"test_data\"\n",
    "    table_id = f\"{PROJECT_ID}.{dataset_name}.{table_name}\"\n",
    "\n",
    "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = bigquery.Dataset(f\"{PROJECT_ID}.{dataset_name}\")\n",
    "    dataset.location = LOCATION.split(\"-\")[0].upper()\n",
    "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[bigquery.SchemaField(field, \"FLOAT\") for field in fields],\n",
    "        write_disposition=\"WRITE_TRUNCATE\"  # Overwrite the table\n",
    "    )\n",
    "        \n",
    "    # Upload data to BigQuery\n",
    "    job = bq_client.load_table_from_dataframe(data_test, table_id, job_config=job_config)\n",
    "    job.result()  # Waits for the job to complete\n",
    "    \n",
    "    print(f\"Loaded {job.output_rows} rows into {table_id}\")\n",
    "\n",
    "    return dataset_name, table_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_predictions_from_bigquery(dataset_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches predictions from a BigQuery table and returns them as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataset_name (str): The name of the dataset containing the predictions table.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the predictions.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {dataset_name}.predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "fields = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_local_folder = \"model-artifacts\"\n",
    "if not os.path.exists(artifact_local_folder):\n",
    "    os.makedirs(artifact_local_folder)\n",
    "model_filename = \"model.json\"\n",
    "artifact_local_path = os.path.join(artifact_local_folder, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(seed=seed)\n",
    "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "clf.save_model(artifact_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "clf.load_model(artifact_local_path)\n",
    "print(\"Test set predictions: \", clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local HTTP server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_folder = \"app\"\n",
    "if not os.path.exists(app_folder):\n",
    "    os.makedirs(app_folder)\n",
    "app_filepath=os.path.join(app_folder, \"app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $app_filepath\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "import os\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# import your functions\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# load model before running app (to be saved in memory)\n",
    "model_ = xgb.XGBClassifier()\n",
    "model_.load_model(artifact_local_path)\n",
    "\n",
    "try:\n",
    "    HEALTH_ROUTE = os.environ[\"AIP_HEALTH_ROUTE\"]\n",
    "    PREDICTIONS_ROUTE = os.environ[\"AIP_PREDICT_ROUTE\"]\n",
    "except KeyError:\n",
    "    HEALTH_ROUTE = \"/health\"\n",
    "    PREDICTIONS_ROUTE = \"/predictions\"\n",
    "\n",
    "def preprocess(content):\n",
    "    preprocessed_content = np.asarray(content)\n",
    "    return preprocessed_content\n",
    "\n",
    "def prediction(ingest_data):\n",
    "    return model_.predict(ingest_data)\n",
    "\n",
    "def postprocess(predictions):\n",
    "    predictions = predictions.tolist()\n",
    "    return {\"predictions\": predictions}\n",
    "\n",
    "@app.get(HEALTH_ROUTE, status_code=200)\n",
    "def health():\n",
    "    return {\"Server is up and running!\"}\n",
    "\n",
    "\n",
    "@app.post(PREDICTIONS_ROUTE)\n",
    "async def predict(request: Request):\n",
    "\n",
    "    request_json = await request.json()\n",
    "    content = request_json[\"instances\"]\n",
    "\n",
    "    ingest_data = preprocess(content)\n",
    "\n",
    "    predictions = prediction(ingest_data)\n",
    "\n",
    "    output = postprocess(predictions)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Using the `%%writefile` magic command does not allow the use of local variables through the `$local_variable` syntax. This is why we need to manually replace `artifact_local_path` with the actual path in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_variable_in_file(app_filepath, r\"artifact_local_path\", artifact_local_path)\n",
    "\n",
    "# Check if replacement was successful\n",
    "!cat $app_filepath -n | grep $artifact_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the fastapi server on a subprocess\n",
    "fastapi_process = subprocess.Popen([\"fastapi\", \"run\"])\n",
    "# This is necessary because if we run it from a cell it will be running indefinetly\n",
    "\n",
    "#wait 30 seconds for http server to start\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"instances\": X_test.tolist()\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/predictions\", json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response status code: \", response.status_code)\n",
    "content_json = json.loads(response.content)\n",
    "print(\"Response content: \", content_json)\n",
    "df_local = pd.DataFrame(X_test, columns=fields)\n",
    "df_local[\"prediction\"] = content_json[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the subprocess\n",
    "fastapi_process.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container HTTP server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_filepath = os.path.join(app_folder, \"requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $requirements_filepath\n",
    "fastapi[standard]\n",
    "scikit-learn\n",
    "xgboost>=1.7.0<2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.10\n",
    "COPY ./app /app\n",
    "COPY ./model-artifacts /model-artifacts\n",
    "\n",
    "RUN pip install -r /app/requirements.txt\n",
    "\n",
    "CMD [\"fastapi\", \"run\"]\n",
    "EXPOSE 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build docker\n",
    "!docker build -t xgboost-predictor ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the container in a subprocess\n",
    "docker_process = subprocess.Popen([\"docker\", \"run\", \"-p\", \"8000:8000\", \"xgboost-predictor\"])\n",
    "\n",
    "#wait 30 seconds for http server to start\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If you are running in WSL2 with docker desktop you need to configure the .wslconfig file to use networking in mirrored mode. This allows the WSL2 kernel to access the localhost as otherwise it will not be able to access the localhost directly but through the mDNS.\n",
    "https://stackoverflow.com/questions/64763147/access-a-localhost-running-in-windows-from-inside-wsl-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"instances\": X_test.tolist()\n",
    "}\n",
    "\n",
    "response = requests.post(f\"http://localhost:8000/predictions\", json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response status code: \", response.status_code)\n",
    "\n",
    "content_json = json.loads(response.content)\n",
    "print(\"Response content: \", content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_process.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Artifact Registry\n",
    "\n",
    "To push the container to the Artifact Registry we need to follow these steps:\n",
    "1. Build the container. (Already done in the previous section)\n",
    "2. Enable the Artifact Registry API.\n",
    "3. Create the repository in Artifact Registry.\n",
    "4. Configure docker to be authenticated to push to Artifact Registry.\n",
    "2. Tag the container.\n",
    "3. Push the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $repository --repository-format=docker \\\n",
    "    --location=$LOCATION \\\n",
    "    --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories list --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure docker to authenticate with the repository endpoint\n",
    "!gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the image\n",
    "!docker tag xgboost-predictor $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the image\n",
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add to model registry\n",
    "\n",
    "To add the model to the model registry we need to follow these steps:\n",
    "1. Enable the AI Platform API.\n",
    "2. Add the Artifact Registry reader role to the Vertex AI service account.\n",
    "3. Upload the model to the Model Registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
    "vertex_ai_service_agent = f\"service-{project_number[0]}@gcp-sa-aiplatform.iam.gserviceaccount.com\"\n",
    "\n",
    "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:$vertex_ai_service_agent\"\\\n",
    "    --role=\"roles/artifactregistry.reader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_display_name = \"xgboost-iris-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check local deployment (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Check the model locally\n",
    "local_model = LocalModel(\n",
    "    serving_container_image_uri=image_uri,\n",
    "    serving_container_predict_route=\"/predictions\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_ports=[8000]\n",
    ")\n",
    "\n",
    "request_dict = dict(instances=X_test.tolist())   \n",
    "json_request = json.dumps(request_dict) \n",
    "\n",
    "# Deploy the model locally and make a prediction\n",
    "with local_model.deploy_to_local_endpoint(\n",
    "    host_port=\"8000\"\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=json_request,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "print(predict_response, predict_response.content)\n",
    "df_custom_local = pd.DataFrame(X_test, columns=fields)\n",
    "df_custom_local[\"prediction\"] = json.loads(predict_response.content)[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    display_name=model_display_name,\n",
    "    serving_container_image_uri=image_uri,\n",
    "    serving_container_predict_route=\"/predictions\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_ports=[8000],\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)\n",
    "\n",
    "# Print the response\n",
    "print(\"Model uploaded successfully:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Vertex AI batch prediction requires data to come either from a GCS bucket or a BigQuery table. In this case, we will copy the data to a BigQuery table. This alternative is useful as we can check the data from the BigQuery console and inspect it before running the batch prediction or after in case we have any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to copy\n",
    "data_test = pd.DataFrame(X_test, columns=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the upload_to_bigquery utility defined in Setup -> Utils\n",
    "dataset_name, table_id = upload_to_bigquery(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model id\n",
    "model_id = !gcloud ai models list --region=$LOCATION --format=\"value(name)\" --filter=\"display_name=$model_display_name\"\n",
    "print(\"Command output: \", model_id)\n",
    "model_id = model_id[-1]\n",
    "print(\"Model ID: \", model_id)\n",
    "try:\n",
    "    int(model_id)\n",
    "except ValueError:\n",
    "    print(\"model_id is not an integer\")\n",
    "    raise Exception(\"Seems the model ID could not be fetched!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resource_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/models/{model_id}\"\n",
    "bigquery_source_input_uri = f\"bq://{table_id}\"\n",
    "bigquery_destination_output_uri = f\"bq://{PROJECT_ID}.{dataset_name}.predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=\"iris_batch_prediction\",\n",
    "    bigquery_source=bigquery_source_input_uri,\n",
    "    bigquery_destination_prefix=bigquery_destination_output_uri,\n",
    "    machine_type=\"e2-standard-2\",\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "batch_prediction_job.wait()\n",
    "\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fetch_predictions_from_bigquery utility defined in Setup -> Utils\n",
    "df = fetch_predictions_from_bigquery(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions:\")\n",
    "print(df[\"prediction\"].astype(int).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The predictions are not in the same order as the input data for the batch prediction call. This is because the data is processed in multiple calls in an asynchronous operation and the order of the predictions is not guaranteed. We could force the order by setting the batch size to the size of the test set, so that it is processed in a single batch. Here we instead just postprocess the predictions to match the order of the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data_test to df\n",
    "df_ordered = pd.merge(df, data_test, how=\"right\")\n",
    "print(\"Ordered Predictions:\")\n",
    "print(df_ordered[\"prediction\"].astype(int).tolist())\n",
    "df_docker = df_ordered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Prediction Routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpr_folder = \"custom_prediction_routine\"\n",
    "bucket_name= f\"iris-bucket-{PROJECT_ID}\"\n",
    "bucket_cpr_path = f\"gs://{bucket_name}/{cpr_folder}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gcs bucket\n",
    "!gsutil mb -l $LOCATION gs://$bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model.json to cloud storage\n",
    "!gsutil cp $artifact_local_path $bucket_cpr_path/$model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list contents of the bucket folder\n",
    "!gsutil ls $bucket_cpr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_uri = f\"{bucket_cpr_path}/{model_filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(cpr_folder):\n",
    "    os.makedirs(cpr_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $cpr_folder/requirements.txt\n",
    "fastapi[standard]\n",
    "scikit-learn\n",
    "xgboost>=1.7.0<2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $cpr_folder/predictor.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud.aiplatform.prediction.sklearn.predictor import SklearnPredictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "class CprPredictor(SklearnPredictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def load(self, artifacts_uri: str):\n",
    "        \"\"\"Loads the preprocessor artifacts.\"\"\"\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "\n",
    "        self._model = xgb.XGBClassifier()\n",
    "        self._model.load_model(\"model.json\")\n",
    "    \n",
    "    def preprocess(self, prediction_input):\n",
    "        instances  = np.asarray(prediction_input[\"instances\"])\n",
    "        return np.asarray(instances)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        prediction_results = self._model.predict(inputs)\n",
    "        return prediction_results\n",
    "    \n",
    "    def postprocess(self, prediction_results):\n",
    "        predictions = prediction_results.tolist()\n",
    "        return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CprPredictor from cpr_folder\n",
    "try:\n",
    "    current_directory = os.getcwd()\n",
    "    os.chdir(cpr_folder)\n",
    "    from predictor import CprPredictor\n",
    "except ImportError:\n",
    "    print(\"Error importing the CPR predictor\")\n",
    "finally:\n",
    "    os.chdir(current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the LocalModel build_cpr_model method. This will build the custom prediction routine docker image.\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    cpr_folder,\n",
    "    image_uri,\n",
    "    base_image=\"python:3.10\",\n",
    "    predictor=CprPredictor,\n",
    "    requirements_path=os.path.join(cpr_folder, \"requirements.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_dict = dict(instances=X_test.tolist())   \n",
    "json_request = json.dumps(request_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model locally and make a prediction\n",
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=artifact_local_folder, # You can also use the GCS path here\n",
    "    host_port=\"8000\"\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=json_request,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_response, predict_response.content)\n",
    "df_cpr_local = pd.DataFrame(X_test, columns=fields)\n",
    "df_cpr_local[\"prediction\"] = json.loads(predict_response.content)[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    local_model=local_model,\n",
    "    display_name=model_display_name,\n",
    "    artifact_uri=bucket_cpr_path,\n",
    "    parent_model=model_resource_name, # This allows us to upload this model as a new version of the existing model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete bigquery prediction table, otherwise the results will be appended\n",
    "with bigquery.Client() as bq_client:\n",
    "    job = bq_client.delete_table(f\"{PROJECT_ID}.{dataset_name}.predictions\", not_found_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=\"iris_batch_prediction\",\n",
    "    bigquery_source=bigquery_source_input_uri,\n",
    "    bigquery_destination_prefix=bigquery_destination_output_uri,\n",
    "    machine_type=\"e2-standard-2\",\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "batch_prediction_job.wait()\n",
    "\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_predictions_from_bigquery(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions:\")\n",
    "print(df[\"prediction\"].astype(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data_test to df\n",
    "ordered_df = pd.merge(df, data_test, how=\"right\")\n",
    "print(\"Ordered Predictions:\")\n",
    "print(ordered_df[\"prediction\"].astype(int).tolist())\n",
    "df_cpr = ordered_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prebuilt Container\n",
    "\n",
    "The required steps to use the prebuilt container are:\n",
    "1. Transform the model to the booster format and upload the artifacts to a GCS bucket.\n",
    "2. Create a model resource in Vertex AI Model Registry.\n",
    "3. Test a batch prediction.\n",
    "\n",
    "> Note: The xgboost prebuilt container requires that we transform the model to the booster format. Previously we have been using the sklearn wrapper, which has a nicer interface but does not allow us to use the prebuilt container. In this section we will transform the model to the booster format and then use the prebuilt container to make a batch prediction. Please take a look at the Vertex AI [documentation](https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#xgboost) for more information on the prebuilt containers requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_image_uri = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the xgboost model to the booster format\n",
    "\n",
    "# load xgboost model\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.load_model(artifact_local_path)\n",
    "\n",
    "#convert to plain xgboost model (not sklearn wrapper)\n",
    "# ref: \n",
    "clf = clf.get_booster()\n",
    "\n",
    "# save model\n",
    "booster_filename = \"model.bst\"\n",
    "booster_model_filepath = os.path.join(artifact_local_folder, booster_filename)\n",
    "clf.save_model(booster_model_filepath)\n",
    "\n",
    "# save to cloud storage\n",
    "booster_folder=\"booster\"\n",
    "booster_folder_uri = f\"gs://{bucket_name}/{booster_folder}\"\n",
    "booster_model_uri = os.path.join(booster_folder_uri, booster_filename)\n",
    "!gsutil cp $booster_model_filepath $booster_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Test it locally\n",
    "\n",
    "local_model = LocalModel(\n",
    "    serving_container_image_uri=prebuilt_image_uri\n",
    ")\n",
    "\n",
    "# **Note: For some reason running this as local expects the model.bst to be in a folder named model\n",
    "local_booster_model_uri = os.path.join(booster_folder_uri, \"model\", booster_filename)\n",
    "!gsutil cp $booster_model_filepath $local_booster_model_uri\n",
    "\n",
    "# Deploy the model locally and make a prediction\n",
    "with local_model.deploy_to_local_endpoint(\n",
    "     # The prebuilt container does not automatically pick up the ADC\n",
    "    credential_path=os.path.expanduser(\"~/.config/gcloud/application_default_credentials.json\"),\n",
    "    # The prebuilt container expects the model to be in a folder named model inside the artifact_uri\n",
    "    # Using a local folder here fails, hence we are using the GCS path here for the local model\n",
    "    artifact_uri=booster_folder_uri,\n",
    "    host_port=\"8000\"\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=json_request,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        verbose=True,\n",
    "    )\n",
    "    sleep(1000)\n",
    "\n",
    "print(predict_response, predict_response.content)\n",
    "df_custom_local = pd.DataFrame(X_test, columns=fields)\n",
    "df_custom_local[\"prediction\"] = json.loads(predict_response.content)[\"predictions\"]\n",
    "\n",
    "!gsutil rm -r $local_booster_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a gcp vertex prebuilt container \n",
    "model = aiplatform.Model.upload(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    display_name=model_display_name,\n",
    "    serving_container_image_uri=prebuilt_image_uri,\n",
    "    parent_model=model_resource_name, # This allows us to upload this model as a new version of the existing model\n",
    "    artifact_uri=booster_folder_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete bigquery prediction table, otherwise the results will be appended\n",
    "with bigquery.Client() as bq_client:\n",
    "    job = bq_client.delete_table(f\"{PROJECT_ID}.{dataset_name}.predictions\", not_found_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch prediction\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=\"iris_batch_prediction\",\n",
    "    bigquery_source=bigquery_source_input_uri,\n",
    "    bigquery_destination_prefix=bigquery_destination_output_uri,\n",
    "    machine_type=\"e2-standard-2\",\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_predictions_from_bigquery(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The prebuilt container returns the predictions scores (i.e. the predicted probabilities for each class), therefore we need to postprocess the predictions to get the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert prediction column to array\n",
    "def get_prediction(x):\n",
    "    float_array = [float(a) for a in x[1:-2].split(\",\")]\n",
    "    label = np.argmax(float_array)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw Predictions:\")\n",
    "print(df[\"prediction\"].tolist())\n",
    "print(\"Postprocessed Predictions:\")\n",
    "print(df[\"prediction\"].apply(get_prediction).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()\n",
    "new_df[\"prediction\"] = new_df[\"prediction\"].apply(get_prediction)\n",
    "ordered_df= pd.merge(new_df, data_test, how=\"right\")\n",
    "print(\"Ordered Predictions:\")\n",
    "print(ordered_df[\"prediction\"].astype(int).tolist())\n",
    "df_prebuilt = ordered_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare results from each df on a single table\n",
    "df_local[\"source\"] = \"local\"\n",
    "df_docker[\"source\"] = \"docker\"\n",
    "df_cpr_local[\"source\"] = \"cpr_local\"\n",
    "df_cpr[\"source\"] = \"cpr\"\n",
    "df_prebuilt[\"source\"] = \"prebuilt\"\n",
    "\n",
    "df_all = pd.concat([df_local, df_docker, df_cpr_local, df_cpr, df_prebuilt])\n",
    "df_all[\"prediction\"] = df_all[\"prediction\"].astype(int)\n",
    "df_pivot = df_all.pivot_table(index=fields, columns=\"source\", values=\"prediction\", aggfunc=\"first\")\n",
    "# add a column checking if all have the same value\n",
    "df_pivot[\"all_equal\"] = df_pivot.apply(lambda x: len(set(list(x)))==1, axis=1)\n",
    "# print length\n",
    "print(\"df_piviot.shape: \", df_pivot.shape)\n",
    "print(\"Value counts of all_equal:\")\n",
    "print(df_pivot[\"all_equal\"].value_counts())\n",
    "df_pivot.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_resources = True\n",
    "\n",
    "if delete_resources:\n",
    "    # delete local files\n",
    "    !rm -r $cpr_folder\n",
    "    !rm -r $artifact_local_folder\n",
    "    !rm -r $app_folder\n",
    "    !rm Dockerfile\n",
    "    !rm environment.yml\n",
    "    # delete bucket\n",
    "    !gsutil rm -r gs://$bucket_name\n",
    "    # delete docker images\n",
    "    !docker rmi xgboost-predictor\n",
    "    !docker rmi $image_uri --force\n",
    "    # delete bigquery dataset\n",
    "    !bq rm -r -f $PROJECT_ID:$dataset_name\n",
    "    # delete model\n",
    "    !gcloud ai models delete $model_id --region=$LOCATION --quiet\n",
    "    # delete artifact registry repository\n",
    "    !gcloud artifacts repositories delete $repository --location=$LOCATION --quiet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pred-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
